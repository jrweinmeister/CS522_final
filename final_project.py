# -*- coding: utf-8 -*-
"""Final Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oQ6_gBQPhscuKoFYmMw3iLH1RKEnIwOh

Mount drive for access to data.
"""

# from google.colab import drive
# drive.mount('/content/gdrive')

"""## Import Libraries"""

import os
import numpy as np
import pandas as pd
import scipy as sc
from scipy import signal
import matplotlib.pyplot as plt
import scipy.io.wavfile as wavfile
import cv2
import librosa

# Import sci-kit models
from sklearn.preprocessing import OneHotEncoder, RobustScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import confusion_matrix

"""## Load Data"""

# root_path = 'gdrive/Shareddrives/COSC 522/Data/'  #change dir to your project folder
root_path = os.getcwd()

# Create data lists
samples = []
labels = []
classes = os.listdir(root_path)

for file in os.listdir(root_path):
  print(file)
  for filename in os.listdir(root_path+file):
    dir_name = root_path+file+"/"
    print(filename)

    # Define sample rate
    fs = 44100

    fs_data, y = wavfile.read(os.path.join(dir_name, filename))
    if len(y.shape) == 2:
        y = y[:,0]
    print(fs_data)
    print(y[2570])
    print(type(y))

    if fs_data == fs:
        labels.append([file])
        samples.append(y)
    else:
        print('Warning: Data in {} not sampled at 44.1 kHz!'.format(filename))
        print(f'Sampled at {fs_data}')
        # TODO: Down sample data that is not 44.1 kHz (most seem to be 48.0 kHz)

    print()
  # break

print(len(samples))
print(len(labels))

"""## Encode labels"""

# Reshape classes list
unique_classes = [[c] for c in classes]

# Create encoder, fit, and check encoding
enc = OneHotEncoder()
enc.fit(unique_classes)
enc_labels = enc.transform(labels).toarray()
print('Encoding is:')
print(enc_labels)

for sample in samples:
  print(len(sample)/44100)

"""## Get Spectrograms"""

def get_spectrograms(time_data, FFT_SIZE=1024, fs=44100):
    ## Compute Fast Fourier Transform of data
    # Create FFT lists
    pxx = []

    print("Calculating FFTs for data ...")
    for sample in time_data:
        # flatten() is used because some of the data is stereo rather than mono. This results in having
        # two data points for each data point rather than 1, and something like (# data points, 2) vs. (# data points, ).
        # flatten(), flattens out the second dimension, resulting in (2x # data points, )
#             if windows is True:
        _, _, pxx_idx = signal.spectrogram(sample, nperseg=FFT_SIZE, fs=fs, noverlap=FFT_SIZE/2)

        pxx.append(pxx_idx)

    print('Calculated all FFTs')

    return pxx.copy()

specs = get_spectrograms(samples)

print(len(specs))
print(len(specs[0]))
print(len(specs[0][0]))

"""## Binning Data"""

def get_bins(spec_data, num_freq_bins=5, num_time_bins=5):
    binned_data = []

    # Get individual samples
    for s in spec_data:
        #Open CV's resize takes (columns,rows) as the input for desired size
        resized_pxx=cv2.resize(s[:,:],(num_time_bins,num_freq_bins))
        binned_data.append(resized_pxx.flatten())

    return binned_data

binned_data = get_bins(specs)

b = np.array(binned_data)
print(b.shape)

"""## Get Freq-Windows"""

def get_freq_windows(time_data, len_window=1, overlap=0, sample_rate=10):
    len_data = time_data.shape[1]
#     print(len_data)

    windows = []

    # How far the window should shift
    if overlap == 0:
        step = len_window

        start = 0
        stop = step
#         print(len_window)
        for n in range(int(len_data/len_window)):
            windows.append(time_data[:, start:stop])

#             print(start)
#             print(stop)
#             print()
            start = start + step
            stop = stop + step

            if start > len_data or stop > len_data:
                break
    else:
        step = int(len_window*overlap)

        start = 0
        stop = len_window

#         print(len_window)
        for n in range(int(len_data/step)):
            windows.append(time_data[:, start:stop])

#             print(start)
#             print(stop)
#             print()
            start = start + step
            stop = stop + step

            if start > len_data or stop > len_data:
                break

    return windows

windowed_freq = []
for fft in specs:
    window_freq_data = get_windows(fft, len_window=2, overlap=.5)
    #shape = np.array(window_data).shape
    #window_data = np.array(window_data).reshape(shape[0], shape[1]*shape[2])
    print(np.array(window_data).shape)
    windowed_freq.append(window_data)

"""## Get Time-Windows"""

def get_time_windows(time_data, len_window_sec=1, overlap=0):
  len_data = time_data.shape[0]
  # print(len_data)

  len_window = len_window_sec*44100

  windows = []
  # How far the window should shift
  if overlap == 0:
      step = len_window

      start = 0
      stop = step
      # print(len_window)

      for n in range(int(len_data/len_window)):
          windows.append(time_data[start:stop])

          # print(start)
          # print(stop)
          # print()
          start = start + step
          stop = stop + step

          if start > len_data or stop > len_data:
              break
  else:
      step = int(len_window*overlap)

      start = 0
      stop = len_window

  #         print(len_window)
      for n in range(int(len_data/step)):
          windows.append(time_data[start:stop])

  #             print(start)
  #             print(stop)
  #             print()
          start = start + step
          stop = stop + step

          if start > len_data or stop > len_data:
              break

  return windows

time_windows = []
num_labels = []
for sample in samples:
  new_windows = get_time_windows(samples[0], len_window_sec=5)
  num_labels.append(len(new_windows))
  print(np.array(new_windows).shape)
  time_windows = time_windows + new_windows

new_labels = []
for label in labels:
  new_labels = new_labels + [label for num in num_labels]

enc_labels = enc.transform(new_labels).toarray()
print('Encoding is:')
print(enc_labels)

train, test = cv.split(np.array(X), np.array(y.argmax(axis=1)))

"""## Feature Extraction"""

specs_reshape = []
for i in range(len(specs)):
  shape = np.array(specs[i]).shape
  X = np.array(specs[i]).reshape(shape[0]*shape[1])
  specs_reshape.append(X)

window_numbers = 20 ## need to be determined
domain_fv = []

for i in range(len(specs_reshape)):

    # MFCC
    mfccs = librosa.feature.mfcc(y=specs_reshape[i], sr=44100, n_mfcc=20, win_length = int(np.ceil(1024/window_numbers)), hop_length = int(np.ceil(1024/(2*window_numbers))))
    mfccs_mean = np.mean(mfccs.T, axis=0)
    #mfccs_mean=mfccs_mean.reshape(513,20)

    # Spectral Centroid
#     sc = librosa.feature.spectral_centroid(y=spectrograms[i], sr=44100, win_length = int(np.ceil(1024/5)), hop_length = int(np.ceil(1024/10)))
#     reshape_sc=[]
#     for x in sc:
#         for j in x:
#             reshape_sc.append(j)

    #all_features=list(zip(mfccs_mean,reshape_sc))
    domain_fv.append(mfccs_mean)



"""## Train Model"""

# Specify which data to use, these are the only parameters that should change, the rest should remain the same.
X = time_windows
y = enc_labels
depth = 5


# # Create our imputer to replace missing values with the mean
# imp = SimpleImputer(missing_values=np.nan, strategy='mean')
# imp = imp.fit(X)

# # Impute our data
# X_imp = imp.transform(X)

# Define model
clf = RandomForestClassifier(max_depth=depth, random_state=0)

# Define number of folds
cv = StratifiedKFold(n_splits=10, shuffle=False)

# Split data, train and test model on 10 folds
split = 1
scores = []
confuse_mat = []
for train_index, test_index in cv.split(np.array(X), np.array(y.argmax(axis=1))):
    print(f"Split {split}/10...")
    x_train, y_train = X[train_index], enc_labels[train_index]
    x_test, y_test = X[test_index], enc_labels[test_index]


    # Fit model and evaluate it
    clf.fit(x_train, y_train)
    scores.append(clf.score(x_test, y_test))

    # Construct confusion matrix
    y_predict = clf.predict(x_test)
    confuse_mat.append(confusion_matrix(y_test.argmax(axis=1), y_predict.argmax(axis=1)))

    # Iterate
    split = split + 1

print("Mean accuracy:" + str(np.mean(scores)))

# TODO: should probably do kfold but doing this just for testing purposes
from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(binned_data, enc_labels, test_size=.3, stratify=enc_labels)

clf = RandomForestClassifier()

clf.fit(x_train, y_train)

print(clf.score(x_test, y_test))

"""## Save Model"""

import pickle

pkl_filename = "pickle_model.pkl"

# Note, for now this will only store the file in volitle memory in the notebook
# To save permanently, most add file path to drive
with open(pkl_filename, 'wb') as file:
  pickle.dump(clf, file)

"""## Load Model"""

with open(pkl_filename, 'rb') as file:
  pickle_model = pickle.load(file)

# Test saved model
print(pickle_model.score(x_test, y_test))